{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    import h5py\n",
    "\n",
    "import bcolz\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras.engine import InputSpec\n",
    "from keras.layers import Input, InputLayer, Lambda, Layer\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit memory used by Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre/post processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1, 1, 3))\n",
    "\n",
    "# Function to subtract imagenet mean and transpose RGB to BGR\n",
    "preproc = lambda x: (x - vgg_mean)[:, :, :, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define convolutional part of VGG16 model, with lamdba layer to pre-process input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_input_layer = InputLayer((288, 288, 3))\n",
    "\n",
    "def add_convolutional_layers(model):\n",
    "    blocks = [\n",
    "        (2, 64),\n",
    "        (2, 128),\n",
    "        (3, 256),\n",
    "        (3, 512),\n",
    "        (3, 512)]\n",
    "    for b in range(len(blocks)):\n",
    "        block = blocks[b]\n",
    "        layers = block[0]\n",
    "        filters = block[1]\n",
    "        prefix = 'block' + str(b + 1)\n",
    "        for i in range(layers):\n",
    "            name = prefix + '_conv' + str(i + 1)\n",
    "            model.add(Conv2D(filters, (3, 3), activation='relu', padding='same', name=name))\n",
    "        name = prefix + '_pool'\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2), name=name))\n",
    "\n",
    "vgg = Sequential()\n",
    "vgg.add(vgg_input_layer)\n",
    "vgg.add(Lambda(preproc, name='lambda'))\n",
    "add_convolutional_layers(vgg)\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable=False\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'https://github.com/fchollet/deep-learning-models'\n",
    "weights_url = repo + '/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "local_name = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "weights_path = get_file(local_name, weights_url, cache_subdir='models')\n",
    "vgg.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 2000\n",
    "arr_hr = bcolz.open('data/imagenet/sample/resized-288.bc')[:num_images]\n",
    "shp = arr_hr.shape[1:]\n",
    "shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = Image.open('data/neural-style/starry_night.png')\n",
    "style_arr = np.array(style)[:shp[0], :shp[1], :shp[2]]\n",
    "plt.imshow(style_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use content loss to create a style-transfer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection padding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        \n",
    "    def compute_output_shape(self, s):\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "    \n",
    "inp = Input((288,288,3))\n",
    "ref_model = Model(inp, ReflectionPadding2D((40,10))(inp))\n",
    "ref_model.compile('adam', 'mse')\n",
    "p = ref_model.predict(arr_hr[10:11])\n",
    "plt.imshow(p[0].astype('uint8'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define upsampling network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, size, stride=(2,2), mode='same', act=True):\n",
    "    x = Conv2D(filters, (size, size), strides=stride, padding=mode)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x) if act else x\n",
    "\n",
    "def res_crop_block(ip, nf=64):\n",
    "    x = conv_block(ip, nf, 3, (1,1), 'valid')\n",
    "    x = conv_block(x,  nf, 3, (1,1), 'valid', False)\n",
    "    ip = Lambda(lambda x: x[:, 2:-2, 2:-2])(ip)\n",
    "    return layers.add([x, ip])\n",
    "\n",
    "def up_block(x, filters, size):\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    x = Conv2D(filters, (size, size), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "inp = Input(shp)\n",
    "x = ReflectionPadding2D((40, 40))(inp)\n",
    "x = conv_block(x, 64, 9, (1,1))\n",
    "x = conv_block(x, 64, 3)\n",
    "x = conv_block(x, 64, 3)\n",
    "for i in range(5): \n",
    "    x = res_crop_block(x)\n",
    "x = up_block(x, 64, 3)\n",
    "x = up_block(x, 64, 3)\n",
    "x = Conv2D(3, (9, 9), activation='tanh', padding='same')(x)\n",
    "outp = Lambda(lambda x: (x + 1) * 127.5)(x)\n",
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outp(m, ln):\n",
    "    return m.get_layer('block{}_conv1'.format(ln)).output\n",
    "\n",
    "vgg_content = Model(vgg_input_layer.input, [get_outp(vgg, o) for o in [2, 3, 4, 5]])\n",
    "vgg_content.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg1 = vgg_content(vgg_input_layer.input)\n",
    "vgg2 = vgg_content(outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targs = [K.variable(o) for o in\n",
    "               vgg_content.predict(np.expand_dims(style_arr, 0))]\n",
    "\n",
    "[K.eval(K.shape(o)) for o in style_targs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sqr_b(diff): \n",
    "    dims = list(range(1, K.ndim(diff)))\n",
    "    return K.expand_dims(K.sqrt(K.mean(diff**2, dims)), 0)\n",
    "\n",
    "def gram_matrix_b(x):\n",
    "    x = K.permute_dimensions(x, (0, 3, 1, 2))\n",
    "    s = K.shape(x)\n",
    "    feat = K.reshape(x, (s[0], s[1], s[2]*s[3]))\n",
    "    return K.batch_dot(feat, K.permute_dimensions(feat, (0, 2, 1))) / K.prod(K.cast(s[1:], K.floatx()))\n",
    "\n",
    "w = [0.1, 0.2, 0.6, 0.1]\n",
    "\n",
    "def tot_loss(x):\n",
    "    loss = 0; n = len(style_targs)\n",
    "    for i in range(n):\n",
    "        loss += mean_sqr_b(gram_matrix_b(x[i+n]) - gram_matrix_b(style_targs[i])) / 2.\n",
    "        loss += mean_sqr_b(x[i]-x[i+n]) * w[i]\n",
    "    return loss\n",
    "\n",
    "loss = Lambda(tot_loss)(vgg1 + vgg2)\n",
    "m_style = Model([inp, vgg_input_layer.input], loss)\n",
    "target = np.zeros((arr_hr.shape[0], 1))\n",
    "m_style.compile('adam', 'mae')\n",
    "epochs = 8\n",
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_style.fit([arr_hr, arr_hr], target, batch_size=batch_size, epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract part of the model that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Model(inp, outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = top_model.predict(arr_hr[:20])\n",
    "plt.imshow(p[0].astype('uint8'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(arr_hr[0].astype('uint8'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
