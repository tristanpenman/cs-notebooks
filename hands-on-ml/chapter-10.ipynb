{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron is one of the earliest, and simplest, neural network architectures. It is based on a kind of artificial neuron called a Linear Threshold Unit (LTU).\n",
    "\n",
    "The inputs and ouputs are numbers (typically {0, 1}, or {-1, 0, 1}, but sometimes real numbers). An LTU computes a weighted sum of its inputs, and applies a step function, typically _heaviside_ or _sign_.\n",
    "\n",
    "A Perceptron is composed of a single layer of LTUs, where each neuron is connected to all inputs (possibly including a bias input, set to 1). Each LTU has exactly one output, so a Perceptron with N outputs will have N LTUs.\n",
    "\n",
    "Perceptrons are trained using a variation of Hebb's rule (popularised as \"neurons that fire together, wire together\"). More specifically, the Perceptron is given one training instance at a time. It makes its predictions, then for each output that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Learning Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights for the Perceptron at step $s$ are determined by the equation:\n",
    "\n",
    "$${w_{i,j}}^{s} = {w_{i,j}}^{s-1} + \\eta(y_j - \\hat y_j) \\cdot x_i$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $w_{i,j}$ is the connection weight from the $i^{th}$ input neuron and the $j^{th}$ output neuron\n",
    "* $x_i$ is the $i^{th}$ input value\n",
    "* $\\hat{y}_i$ is the output of the $j^{th}$ output neuron\n",
    "* $y_i$ is the target output of the $j^{th}$ output neuron\n",
    "* $\\eta$ is the learning rate\n",
    "\n",
    "We can see how if the target was $y_j=1$, and the output was ${\\hat y}_j=0$, then $(y_j - \\hat y_j)=1$, increasing the magnitude of the weight associated with $x_i$.\n",
    "\n",
    "Similarly, if the target was $y_j=0$, and the output was ${\\hat y}_j=1$, then $(y_j - \\hat y_j)=-1$. This will decrease the magnitude of the weight associated with $x_i$.\n",
    "\n",
    "Note that when $y_j = {\\hat y}_j = 0$, the weights remain unchanged, so it is only the incorrect outputs that influence learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a Perceptron class that implements a single LTU network.\n",
    "\n",
    "Using the Iris dataset, we can train a Perceptron classifier to predict whether or not a sample is an Iris Setosa, based on petal length and petal width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Use petal length and petal width as inputs\n",
    "X = iris.data[:, (2, 3)]\n",
    "\n",
    "# This classifier will identify Iris Setosa (target = 0)\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "# Train a single perceptron\n",
    "per_clf = Perceptron(random_state=42, max_iter=100, tol=-np.infty)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: measure accuracy on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising a perceptron classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: use matplotlib to visualise the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multi-Layer Perceptron (MLP) adds one or more intermediate (hidden) layers to the Perceptron architecture. Every layer except the output layer contains a bias neuron and is fully connected to the next layer. Training of an MLP is usually performed using Backpropagation (explored further in the notebook for Appendix D).\n",
    "\n",
    "When a MLP has two or more hidden layers, it is typically referred to as a Deep Neural Network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn includes a class called `MLPClassifier`, which implements a multi-layer perceptron (MLP) algorithm using Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron with TF Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow includes an estimator API that provides pre-baked implementations of various kinds of estimators, as well as supporting the creation of custom estimators. One of the built in estimators is `DNNClassifier`, which can be used to train a multi-layer perceptron.\n",
    "\n",
    "The following example trains a multi-layer perceptron to classify digits in the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a multi-layer perceptron using TensorFlow's high-level *TF.Learn* API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(\"X\", shape=[28 * 28])]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[300, 100], \n",
    "    n_classes=10, \n",
    "    feature_columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_train},\n",
    "    y=y_train,\n",
    "    num_epochs=40,\n",
    "    batch_size=50,\n",
    "    shuffle=True)\n",
    "\n",
    "dnn_clf.train(input_fn=input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_test},\n",
    "    y=y_test,\n",
    "    shuffle=False)\n",
    "\n",
    "eval_results = dnn_clf.evaluate(input_fn=test_input_fn)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using raw TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        # Randomly initialize weights using a stddev based on number of inputs\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        # Create a subgraph to compute z = X * W + b\n",
    "        z = tf.matmul(X, W) + b\n",
    "        # Optional activation function\n",
    "        if activation == 'relu':\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden2, n_outputs, \"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use TensorFlow's built-in `fully_connected` function to define dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\", activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
